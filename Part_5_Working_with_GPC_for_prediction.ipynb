{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [AI Platform Prediction overview](https://cloud.google.com/ai-platform/prediction/docs/overview)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required modules\n",
    "import glob\n",
    "import os \n",
    "\n",
    "import tensorflow as tf\n",
    "from oauth2client.client import GoogleCredentials\n",
    "from googleapiclient import discovery\n",
    "from googleapiclient import errors\n",
    "from google.cloud import storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "When you want to use the Google APIs Client Library for Python to call the AI Platform Prediction REST APIs in your code, \n",
    "you must import its package and the OAuth2 package. \n",
    "For most standard uses of AI Platform Prediction you only need to import specific modules\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set path to credentials file as environment variables\n",
    "currentDirectory = os.getcwd()\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = os.path.join(currentDirectory, 'toucanassistant-55361b918986.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucket Storage\n",
    "BUCKET_NAME_2=\"toucan-assistant-bucket-2\"\n",
    "REGION_EU_WEST1=\"europe-west1\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Platform Prediction online prediction is currently available in the following regions:\n",
    "  * us-central1\n",
    "  * us-east1\n",
    "  * us-east4\n",
    "  * asia-northeast1\n",
    "  * europe-west1\n",
    "  \n",
    "We have <b>europe-west3</b> in bucket-1, and we have <b>europe-west1</b> n bucket-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set job name and job folder as environment variables\n",
    "os.environ['JOB_NAME']=\"my_second_keras_job\"\n",
    "os.environ['JOB_DIR']=\"gs://$BUCKET_NAME/keras-job-dir-2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Cloud Storage Client Libraries](https://cloud.google.com/storage/docs/reference/libraries?hl=pl#client-libraries-install-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Google Cloud Client Libraries use our latest client library model and are our recommended option for accessing Cloud APIs programmatically, where available. Cloud Client Libraries:\n",
    "\n",
    "  * Provide idiomatic, generated or hand-written code in each language, making the Cloud API simple and intuitive to use.\n",
    "  * Handle all the low-level details of communication with the server, including authenticating with Google.\n",
    "  * Can be installed using familiar package management tools such as npm and pip.\n",
    "  * In some cases, give you performance benefits by using gRPC. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cloud IAM:\n",
    "  * [Cloud Identity and Access Management](https://cloud.google.com/storage/docs/access-control/iam)\n",
    "  * GCP user must have proper role to create buckets (in this case the role is StorageAdmin)\n",
    "  * [Cloud IAM roles for Cloud Storage](https://cloud.google.com/storage/docs/access-control/iam-roles) roles\n",
    "  * You can't create bucket with the name that exists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The name for the new bucket\n",
    "bucket_name_3 = \"toucan-assistant-bucket-3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the new bucket in proper location\n",
    "def create_bucket_in_location(name_of_bucket, location_of_bucket):\n",
    "    # Instantiates a client\n",
    "    storage_client = storage.Client()\n",
    "    # https://stackoverflow.com/questions/54992263/creating-bucket-in-google-cloud-storage-in-custom-location\n",
    "    # https://stackoverflow.com/questions/42576366/google-cloud-storage-python-api-create-bucket-in-specified-location\n",
    "    # bucket name must be unique !!!\n",
    "    custom_bucket = storage_client.bucket(name_of_bucket)\n",
    "    custom_bucket.create(location=location_of_bucket)\n",
    "    print(f'Bucket {name_of_bucket} was created!')\n",
    "\n",
    "create_bucket_in_location(bucket_name_3, REGION_EU_WEST1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting bucket\n",
    "# Be careful or set force=False\n",
    "# force=True deletes non-empty folders !!!\n",
    "def delete_bucket(name_of_bucket):\n",
    "    storage_client = storage.Client()\n",
    "    custom_bucket = storage_client.get_bucket(name_of_bucket)\n",
    "    custom_bucket.delete(force=True)\n",
    "    print(f'Bucket {name_of_bucket} was deleted!')\n",
    "    \n",
    "delete_bucket(bucket_name_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to copying file \n",
    "def upload_file(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    # bucket_name = \"your-bucket-name\"\n",
    "    # source_file_name = \"local/path/to/file\"\n",
    "    # destination_blob_name = \"storage-object-name\"\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print(\n",
    "        \"File {} uploaded to {}.\".format(\n",
    "            source_file_name, destination_blob_name\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on file\n",
    "# Files are overwritten !!!\n",
    "file_to_copy = 'requirements.txt'\n",
    "file_blob_name = 'copied_requirements.txt'\n",
    "upload_file(bucket_name_3, file_to_copy, file_blob_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://hackersandslackers.com/manage-files-in-google-cloud-storage-with-python/\n",
    "def upload_folder(bucketName, localFolder):\n",
    "    \"\"\"Upload files to GCP bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucketName)\n",
    "\n",
    "    files = [f for f in os.listdir(localFolder) if os.path.isfile(os.path.join(localFolder, f))]\n",
    "    for file in files:\n",
    "        localFile = localFolder + '/' + file\n",
    "        blob = bucket.blob(localFolder + '/' + file)\n",
    "        blob.upload_from_filename(localFile)\n",
    "    return f'Uploaded {files} to \"{bucketName}\" bucket.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_local_directory_to_gcs(local_path, bucketName, gcs_path):\n",
    "    assert os.path.isdir(local_path)\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucketName)\n",
    "    \n",
    "    for local_file in glob.glob(local_path + '/**'):\n",
    "        if not os.path.isfile(local_file):\n",
    "           upload_local_directory_to_gcs(local_file, bucket, gcs_path + \"/\" + os.path.basename(local_file))\n",
    "        else:\n",
    "            remote_path = os.path.join(gcs_path, local_file[1 + len(local_path):])\n",
    "            blob = bucket.blob(remote_path)\n",
    "            blob.upload_from_filename(local_file)\n",
    "    print(f'Folder {local_path} uploaded to bucket {bucketName}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on folder\n",
    "folder_to_copy = 'keras_export'\n",
    "folder_blob_name = 'copied_trainer'\n",
    "#upload_folder(bucket_name_3, folder_to_copy)\n",
    "upload_local_directory_to_gcs(folder_to_copy, bucket_name_3, folder_blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st step - [Exporting saved models for prediction](https://cloud.google.com/ai-platform/prediction/docs/exporting-savedmodel-for-prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>A SavedModel is TensorFlow's <b>recommended</b> format for saving models, and it is the required format for deploying trained TensorFlow models on AI Platform Prediction.</p>\n",
    "<p>Exporting your trained model as a SavedModel saves your training graph with its assets, variables and metadata in a format that AI Platform Prediction can consume and restore for predictions.\n",
    "\n",
    "After exporting a SavedModel, you have a SavedModel directory that contains the following:</p>\n",
    "\n",
    "  * your training graph(s), saved in SavedModel protocol buffers\n",
    "  * external files, called assets\n",
    "  * variables, which are saved as checkpoint files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>When you deploy your SavedModel to AI Platform Prediction, you must include the <b>entire SavedModel directory</b>, not just the SavedModel protocol buffer file that contains your graph and its metadata. This file usually has an extension of either .pb or .pbtxt.</p>\n",
    "\n",
    "<p>The SavedModel allows you to save multiple versions of a graph that share the same assets and variables (or checkpoints). For example, you may want to develop two versions of the same graph: one to run on CPUs, and another to run on GPUs.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have used Keras for training, use [tf.keras.Model.save](https://www.tensorflow.org/guide/keras/save_and_serialize#export_to_savedmodel) to export a SavedModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_DIR=\"gs://$BUCKET_NAME/keras-job-dir-2\"\n",
    "JOB_DIR = os.getenv('JOB_DIR')\n",
    "# Export the model to a SavedModel directory in Cloud Storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd step -  [Deploying models](https://cloud.google.com/ai-platform/prediction/docs/deploying-models#rest-api_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to <b>deploy your trained model</b> on AI Platform Prediction, you must:\n",
    "\n",
    "  * Upload your saved model to a Cloud Storage bucket.\n",
    "  * Create an AI Platform Prediction [model](https://cloud.google.com/ai-platform/prediction/docs/reference/rest/v1/projects.models#Model) resource.\n",
    "  * Create an AI Platform Prediction [version](https://cloud.google.com/ai-platform/prediction/docs/reference/rest/v1/projects.models.versions) resource, specifying the Cloud Storage path to your saved model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd step -  [Getting online predictions](https://cloud.google.com/ai-platform/prediction/docs/online-predict#rest-api)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to request predictions, you must first:\n",
    "\n",
    " * Export your trained model as one or more model artifacts that can be deployed to AI Platform Prediction.\n",
    "\n",
    " * Deploy your trained model to AI Platform Prediction by creating a model resource and version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the [Google APIs Client Library for Python](https://cloud.google.com/ai-platform/prediction/docs/python-client-library?hl=sr) to call the AI Platform Prediction REST APIs in your Python applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projectname",
   "language": "python",
   "name": "projectname"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
